{
    "docs": [
        {
            "location": "/", 
            "text": "Maxwell = Mysql + Kafka\n\n\n\n\n\nThis is Maxwell's daemon, an application that reads MySQL binlogs and writes row updates to Kafka as JSON.\nIt's playing in the same space as \nmypipe\n and \ndatabus\n,\nbut differentiates itself with these features:\n\n\n\n\nWorks with an unpatched mysql\n\n\nParses ALTER/CREATE/DROP table statements, which allows Maxwell to always have a correct view of the mysql schema\n\n\nStores its replication position and needed data within the mysql server itself\n\n\nRequires no external dependencies (save Kafka, if used)\n\n\nEschews the complexity of Avro for plain old JSON.\n\n\nMinimal setup\n\n\n\n\nMaxwell is intended as a source for event-based readers, eg various ETL applications, search indexing,\nstat emitters.\n\n\n\n\nDownload: \nhttps://github.com/zendesk/maxwell/releases/download/v1.1.2/maxwell-1.1.2.tar.gz\n\n\nSource: \nhttps://github.com/zendesk/maxwell\n\n\n\n\n\n\n  mysql\n insert into `test`.`maxwell` set id = 1, daemon = 'Stanislaw Lem';\n  maxwell: {\n    \ndatabase\n: \ntest\n,\n    \ntable\n: \nmaxwell\n,\n    \ntype\n: \ninsert\n,\n    \nts\n: 1449786310,\n    \nxid\n: 940752,\n    \ncommit\n: true,\n    \ndata\n: { \nid\n:1, \ndaemon\n: \nStanislaw Lem\n }\n  }\n\n\n\n\n  mysql\n update test.maxwell set daemon = 'firebus!  firebus!' where id = 1;\n  maxwell: {\n    \ndatabase\n: \ntest\n,\n    \ntable\n: \nmaxwell\n,\n    \ntype\n: \nupdate\n,\n    \nts\n: 1449786341,\n    \nxid\n: 940786,\n    \ncommit\n: true,\n    \ndata\n: {\nid\n:1, \ndaemon\n: \nFirebus!  Firebus!\n},\n    \nold\n:  {\ndaemon\n: \nStanislaw Lem\n}\n  }\n\n\n\n\n\n  jQuery(document).ready(function () {\n    jQuery(\"#maxwell-header\").append(\n      jQuery(\"<img alt='The Daemon, maybe' src='./img/cyberiad_1.jpg' id='maxwell-daemon-image'>\")\n    );\n    jQuery(\"pre\").addClass(\"home-code\");\n  });", 
            "title": "Overview"
        }, 
        {
            "location": "/quickstart/", 
            "text": "Download\n\n\n\n\n\n\nDownload binary distro: \nhttps://github.com/zendesk/maxwell/releases/download/v1.1.2/maxwell-1.1.2.tar.gz\n\n\nSources and bug tracking is available on github: \nhttps://github.com/zendesk/maxwell\n\n\nObligatory copy/paste to terminal:\n\n\n\n\ncurl -sLo - https://github.com/zendesk/maxwell/releases/download/v1.1.2/maxwell-1.1.2.tar.gz \\\n       | tar zxvf -\ncd maxwell-1.1.2\n\n\n\n\nRow based replication\n\n\n\n\nMaxwell can only operate if row-based replication is on.\n\n\n$ vi my.cnf\n\n[mysqld]\nserver-id=1\nlog-bin=master\nbinlog_format=row\n\n\n\n\nOr on a running server:\n\n\nmysql\n set global binlog_row_image=FULL;\n\n\n\n\nnote\n: When changing the binlog format on a running server, currently connected mysql clients will continue to replication in STATEMENT format --\nin order to change to row-based replication, you must reconnect all active clients to the server.\n\n\nMysql permissions\n\n\n\n\nMaxwell stores all the state it needs within the mysql server itself, in the database called specified by the \nschema_database\n option. By default the database is named \nmaxwell\n.\n\n\nmysql\n GRANT ALL on maxwell.* to 'maxwell'@'%' identified by 'XXXXXX';\nmysql\n GRANT SELECT, REPLICATION CLIENT, REPLICATION SLAVE on *.* to 'maxwell'@'%';\n\n# or for running maxwell locally:\n\nmysql\n GRANT SELECT, REPLICATION CLIENT, REPLICATION SLAVE on *.* to 'maxwell'@'localhost' identified by 'XXXXXX';\nmysql\n GRANT ALL on maxwell.* to 'maxwell'@'localhost';\n\n\n\n\n\nSTDOUT producer\n\n\n\n\nUseful for smoke-testing the thing.\n\n\nbin/maxwell --user='maxwell' --password='XXXXXX' --host='127.0.0.1' --producer=stdout\n\n\n\n\nIf all goes well you'll see maxwell replaying your inserts:\n\n\nmysql\n insert into test.maxwell set id = 5, daemon = 'firebus!  firebus!';\nQuery OK, 1 row affected (0.04 sec)\n\n(maxwell)\n{\ntable\n:\nmaxwell\n,\ntype\n:\ninsert\n,\ndata\n:{\nid\n:5,\ndaemon\n:\nfirebus!  firebus!\n},\nts\n: 123456789}\n\n\n\n\nKafka producer\n\n\n\n\nBoot kafka as described here:  \nhttp://kafka.apache.org/documentation.html#quickstart\n, then:\n\n\nbin/maxwell --user='maxwell' --password='XXXXXX' --host='127.0.0.1' \\\n   --producer=kafka --kafka.bootstrap.servers=localhost:9092\n\n\n\n\nThis will start writing to the topic \"maxwell\".", 
            "title": "Quick Start"
        }, 
        {
            "location": "/quickstart/#download", 
            "text": "Download binary distro:  https://github.com/zendesk/maxwell/releases/download/v1.1.2/maxwell-1.1.2.tar.gz  Sources and bug tracking is available on github:  https://github.com/zendesk/maxwell  Obligatory copy/paste to terminal:   curl -sLo - https://github.com/zendesk/maxwell/releases/download/v1.1.2/maxwell-1.1.2.tar.gz \\\n       | tar zxvf -\ncd maxwell-1.1.2", 
            "title": "Download"
        }, 
        {
            "location": "/quickstart/#row-based-replication", 
            "text": "Maxwell can only operate if row-based replication is on.  $ vi my.cnf\n\n[mysqld]\nserver-id=1\nlog-bin=master\nbinlog_format=row  Or on a running server:  mysql  set global binlog_row_image=FULL;  note : When changing the binlog format on a running server, currently connected mysql clients will continue to replication in STATEMENT format --\nin order to change to row-based replication, you must reconnect all active clients to the server.", 
            "title": "Row based replication"
        }, 
        {
            "location": "/quickstart/#mysql-permissions", 
            "text": "Maxwell stores all the state it needs within the mysql server itself, in the database called specified by the  schema_database  option. By default the database is named  maxwell .  mysql  GRANT ALL on maxwell.* to 'maxwell'@'%' identified by 'XXXXXX';\nmysql  GRANT SELECT, REPLICATION CLIENT, REPLICATION SLAVE on *.* to 'maxwell'@'%';\n\n# or for running maxwell locally:\n\nmysql  GRANT SELECT, REPLICATION CLIENT, REPLICATION SLAVE on *.* to 'maxwell'@'localhost' identified by 'XXXXXX';\nmysql  GRANT ALL on maxwell.* to 'maxwell'@'localhost';", 
            "title": "Mysql permissions"
        }, 
        {
            "location": "/quickstart/#stdout-producer", 
            "text": "Useful for smoke-testing the thing.  bin/maxwell --user='maxwell' --password='XXXXXX' --host='127.0.0.1' --producer=stdout  If all goes well you'll see maxwell replaying your inserts:  mysql  insert into test.maxwell set id = 5, daemon = 'firebus!  firebus!';\nQuery OK, 1 row affected (0.04 sec)\n\n(maxwell)\n{ table : maxwell , type : insert , data :{ id :5, daemon : firebus!  firebus! }, ts : 123456789}", 
            "title": "STDOUT producer"
        }, 
        {
            "location": "/quickstart/#kafka-producer", 
            "text": "Boot kafka as described here:   http://kafka.apache.org/documentation.html#quickstart , then:  bin/maxwell --user='maxwell' --password='XXXXXX' --host='127.0.0.1' \\\n   --producer=kafka --kafka.bootstrap.servers=localhost:9092  This will start writing to the topic \"maxwell\".", 
            "title": "Kafka producer"
        }, 
        {
            "location": "/config/", 
            "text": "Command line options\n\n\n\n\n\n\n\n\n\n\noption\n\n\ndescription\n\n\ndefault\n\n\n\n\n\n\n\n\n\n\n--config FILE\n\n\nlocation of \nconfig.properties\n file\n\n\n\n\n\n\n\n\n--log_level\n\n\nlog level [DEBUG\nINFO \nWARN\nERROR\n\n\nINFO\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n--host HOST\n\n\nmysql host\n\n\n\n\n\n\n\n\n--user USER\n\n\nmysql username\n\n\n\n\n\n\n\n\n--password PASSWORD\n\n\nmysql password\n\n\n(none)\n\n\n\n\n\n\n--port PORT\n\n\nmysql port\n\n\n3306\n\n\n\n\n\n\n--schema_database\n\n\ndatabase name where maxwell stores schema and state\n\n\nmaxwell\n\n\n\n\n\n\n--max_schemas\n\n\nhow many old schemas maxwell should leave lying around in maxwell.schemas\n\n\n5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n--producer PRODUCER\n\n\nwhat type of producer to use: [stdout, kafka, file, profiler]\n\n\nstdout\n\n\n\n\n\n\n--output_file\n\n\nif using the file producer, write JSON rows to this path\n\n\n\n\n\n\n\n\n--kafka.bootstrap.servers\n\n\nlist of kafka brokers, listed as HOST:PORT[,HOST:PORT]\n\n\n\n\n\n\n\n\n--kafka_partition_hash\n\n\nwhich hash function to use: [default, murmur3]\n\n\ndefault\n\n\n\n\n\n\n--kafka_partition_by\n\n\nwhat fields to hash for partition key: [database, table, primary_key]\n\n\ndatabase\n\n\n\n\n\n\n--kafka_topic\n\n\nkafka topic to write to.\n\n\nmaxwell\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n--replication_host\n\n\nmysql host to replicate from.  Only specify if different from \nhost\n (see notes)\n\n\nschema-store host\n\n\n\n\n\n\n--replication_password\n\n\npassword on replication server\n\n\n(none)\n\n\n\n\n\n\n--replication_port\n\n\nport on replication server\n\n\n3306\n\n\n\n\n\n\n--replication_user\n\n\nuser on replication server\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n--include_dbs PATTERN\n\n\nonly send updates from these databases\n\n\n\n\n\n\n\n\n--exclude_dbs PATTERN\n\n\nignore updates from these databases\n\n\n\n\n\n\n\n\n--include_tables PATTERN\n\n\nonly send updates from tables named like PATTERN\n\n\n\n\n\n\n\n\n--exclude_tables PATTERN\n\n\nignore updates from tables named like PATTERN\n\n\n\n\n\n\n\n\n--blacklist_dbs PATTERN\n\n\nignore updates AND schema changes from databases (see warnings below)\n\n\n\n\n\n\n\n\n--blacklist_tables PATTERN\n\n\nignore updates AND schema changes from tables named like PATTERN (see warnings below)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n--bootstrapper\n\n\nbootstrapper type: async\n\n\nsync\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n--init_position FILE:POSITION\n\n\nignore the information in maxwell.positions and start at the given binlog position. Not available in config.properties.\n\n\n\n\n\n\n\n\n--replay\n\n\nenable maxwell's read-only \"replay\" mode.  Not available in config.properties.\n\n\n\n\n\n\n\n\n\n\nProperties file\n\n\n\n\nIf maxwell finds the file \nconfig.properties\n in $PWD it will use it.  Any\ncommand line options (except init_position and replay) may be given as\n\"key=value\" pairs.\n\n\nAdditionally, any configuration file options prefixed with 'kafka.' will be\npassed into the kafka producer library, after having 'kafka.' stripped off the\nfront of the key.  So, for example if config.properties contains\n\n\nkafka.batch.size=16384\n\n\n\n\nthen Maxwell will send \nbatch.size=16384\n to the kafka producer library.\n\n\nRunning against RDS\n\n\n\n\nTo run Maxwell against RDS, (either Aurora or Mysql) you will need to do the following:\n\n\n\n\nset binlog_format to \"ROW\".  Do this in the \"parameter groups\" section.  For a Mysql-RDS instance this parameter will be\n  in a \"DB Parameter Group\", for Aurora it will be in a \"DB Cluster Parameter Group\".\n\n\nsetup RDS binlog retention as described \nhttp://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_LogAccess.Concepts.MySQL.html\n.\n  The tl;dr is to execute \ncall mysql.rds_set_configuration('binlog retention hours', 24)\n on the server.\n\n\n\n\nFilters\n\n\n\n\nThe options \ninclude_dbs\n, \nexclude_dbs\n, \ninclude_tables\n, and \nexclude_tables\n control whether\nMaxwell will send an update for a given row to its producer.  All the options take a single value PATTERN,\nwhich may either be a literal table/database name, given as \noption=name\n, or a regular expression,\ngiven as \noption=/regex/\n.  The options are evaluated as follows:\n\n\n\n\nonly accept databases in \ninclude_dbs\n if non-empty\n\n\nreject databases in \nexclude_dbs\n\n\nonly accept tables in \ninclude_tables\n if non-empty\n\n\nreject tables in \nexclude_tables\n\n\n\n\nSo an example like \n--include_dbs=/foo.*/ --exclude_tables=bar\n will include \nfooty.zab\n and exclude \nfooty.bar\n\n\nThe option \nblacklist_tables\n and \nblacklist_dbs\n controls whether Maxwell will send updates for a table to its producer AND whether\nit captures schema changes for that table or database. Note that once Maxwell has been running with a table or database marked as blacklisted,\nyou \nmust\n continue to run Maxwell with that table or database blacklisted or else Maxwell will halt. If you want to stop\nblacklisting a table or database, you will have to drop the maxwell schema first.\n\n\nSchema storage host vs replica host\n\n\n\n\nMaxwell needs two sets of mysql permissions to operate properly: a mysql database in which to store schema snapshots,\nand a mysql host to replicate from.  The recommended configuration is that\nthese two functions are provided by a single mysql host.  In this case, just\nspecify \nhost\n, \nuser\n, etc.\n\n\nSome configurations, however, may need to write data to a different server than it replicates from.  In this case,\nthe host described by \nhost\n, \nuser\n, ..., will be used to write schema information to, and Maxwell will replicate\nevents from the host described by \nreplication_host\n, \nreplication_user\n, ...  Note that bootstrapping is not available\nin this configuration.\n\n\n\n  jQuery(document).ready(function () {\n    jQuery(\"table\").addClass(\"table table-condensed table-bordered table-hover\");\n  });", 
            "title": "Configuration"
        }, 
        {
            "location": "/config/#command-line-options", 
            "text": "option  description  default      --config FILE  location of  config.properties  file     --log_level  log level [DEBUG INFO  WARN ERROR  INFO         --host HOST  mysql host     --user USER  mysql username     --password PASSWORD  mysql password  (none)    --port PORT  mysql port  3306    --schema_database  database name where maxwell stores schema and state  maxwell    --max_schemas  how many old schemas maxwell should leave lying around in maxwell.schemas  5         --producer PRODUCER  what type of producer to use: [stdout, kafka, file, profiler]  stdout    --output_file  if using the file producer, write JSON rows to this path     --kafka.bootstrap.servers  list of kafka brokers, listed as HOST:PORT[,HOST:PORT]     --kafka_partition_hash  which hash function to use: [default, murmur3]  default    --kafka_partition_by  what fields to hash for partition key: [database, table, primary_key]  database    --kafka_topic  kafka topic to write to.  maxwell         --replication_host  mysql host to replicate from.  Only specify if different from  host  (see notes)  schema-store host    --replication_password  password on replication server  (none)    --replication_port  port on replication server  3306    --replication_user  user on replication server          --include_dbs PATTERN  only send updates from these databases     --exclude_dbs PATTERN  ignore updates from these databases     --include_tables PATTERN  only send updates from tables named like PATTERN     --exclude_tables PATTERN  ignore updates from tables named like PATTERN     --blacklist_dbs PATTERN  ignore updates AND schema changes from databases (see warnings below)     --blacklist_tables PATTERN  ignore updates AND schema changes from tables named like PATTERN (see warnings below)          --bootstrapper  bootstrapper type: async  sync         --init_position FILE:POSITION  ignore the information in maxwell.positions and start at the given binlog position. Not available in config.properties.     --replay  enable maxwell's read-only \"replay\" mode.  Not available in config.properties.", 
            "title": "Command line options"
        }, 
        {
            "location": "/config/#properties-file", 
            "text": "If maxwell finds the file  config.properties  in $PWD it will use it.  Any\ncommand line options (except init_position and replay) may be given as\n\"key=value\" pairs.  Additionally, any configuration file options prefixed with 'kafka.' will be\npassed into the kafka producer library, after having 'kafka.' stripped off the\nfront of the key.  So, for example if config.properties contains  kafka.batch.size=16384  then Maxwell will send  batch.size=16384  to the kafka producer library.", 
            "title": "Properties file"
        }, 
        {
            "location": "/config/#running-against-rds", 
            "text": "To run Maxwell against RDS, (either Aurora or Mysql) you will need to do the following:   set binlog_format to \"ROW\".  Do this in the \"parameter groups\" section.  For a Mysql-RDS instance this parameter will be\n  in a \"DB Parameter Group\", for Aurora it will be in a \"DB Cluster Parameter Group\".  setup RDS binlog retention as described  http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_LogAccess.Concepts.MySQL.html .\n  The tl;dr is to execute  call mysql.rds_set_configuration('binlog retention hours', 24)  on the server.", 
            "title": "Running against RDS"
        }, 
        {
            "location": "/config/#filters", 
            "text": "The options  include_dbs ,  exclude_dbs ,  include_tables , and  exclude_tables  control whether\nMaxwell will send an update for a given row to its producer.  All the options take a single value PATTERN,\nwhich may either be a literal table/database name, given as  option=name , or a regular expression,\ngiven as  option=/regex/ .  The options are evaluated as follows:   only accept databases in  include_dbs  if non-empty  reject databases in  exclude_dbs  only accept tables in  include_tables  if non-empty  reject tables in  exclude_tables   So an example like  --include_dbs=/foo.*/ --exclude_tables=bar  will include  footy.zab  and exclude  footy.bar  The option  blacklist_tables  and  blacklist_dbs  controls whether Maxwell will send updates for a table to its producer AND whether\nit captures schema changes for that table or database. Note that once Maxwell has been running with a table or database marked as blacklisted,\nyou  must  continue to run Maxwell with that table or database blacklisted or else Maxwell will halt. If you want to stop\nblacklisting a table or database, you will have to drop the maxwell schema first.", 
            "title": "Filters"
        }, 
        {
            "location": "/config/#schema-storage-host-vs-replica-host", 
            "text": "Maxwell needs two sets of mysql permissions to operate properly: a mysql database in which to store schema snapshots,\nand a mysql host to replicate from.  The recommended configuration is that\nthese two functions are provided by a single mysql host.  In this case, just\nspecify  host ,  user , etc.  Some configurations, however, may need to write data to a different server than it replicates from.  In this case,\nthe host described by  host ,  user , ..., will be used to write schema information to, and Maxwell will replicate\nevents from the host described by  replication_host ,  replication_user , ...  Note that bootstrapping is not available\nin this configuration.  \n  jQuery(document).ready(function () {\n    jQuery(\"table\").addClass(\"table table-condensed table-bordered table-hover\");\n  });", 
            "title": "Schema storage host vs replica host"
        }, 
        {
            "location": "/kafka/", 
            "text": "Kafka options\n\n\n\n\nAny options given to Maxwell that are prefixed with \nkafka.\n will be passed directly into the Kafka producer configuration\n(with \nkafka.\n stripped off).  We use the \"new producer\" configuration, as described here:\n\nhttp://kafka.apache.org/documentation.html#newproducerconfigs\n\n\nMaxwell sets the following Kafka options by default, but you can override them in \nconfig.properties\n.\n\n\n\n\nkafka.acks = 1\n\n\nkafka.compression.type = gzip\n\n\n\n\nMaxwell writes to a kafka topic named \"maxwell\" by default.  This can be changed with the \nkafka_topic\n option.\n\n\nKafka key\n\n\n\n\nMaxwell generates keys for its Kafka messages based upon a mysql row's primary key in JSON format:\n\n\n{ \ndatabase\n:\ntest_tb\n,\ntable\n:\ntest_tbl\n,\npk.id\n:4,\npk.part2\n:\nhello\n}\n\n\n\n\nThis key is designed to co-operate with Kafka's log compaction, which will save the last-known\nvalue for a key, allowing Maxwell's Kafka stream to retain the last-known value for a row and act\nas a source of truth.\n\n\nPartitioning\n\n\n\n\nA binlog event's partition is determined by the selected hash function and hash string as follows\n\n\n  HASH_FUNCTION(HASH_STRING) % TOPIC.NUMBER_OF_PARTITIONS\n\n\n\n\nThe HASH_FUNCTION is either java's \nhashCode\n or \nmurmurhash3\n. The default HASH_FUNCTION is \nhashCode\n. Murmurhash3 may be set with the \nkafka_partition_hash\n option. The seed value for the murmurhash function is hardcoded to 25342 in the MaxwellKafkaPartitioner class.\n\n\nThe HASH_STRING may be (\ndatabase\n, \ntable\n, \nprimary_key\n).  The default HASH_STRING is the \ndatabase\n. The partitioning field can be configured using the \nkafka_partition_by\n option.    \n\n\nMaxwell will discover the number of partitions in its kafka topic upon boot.  This means that you should pre-create your kafka topics,\nand with at least as many partitions as you have logical databases:\n\n\nbin/kafka-topics.sh --zookeeper ZK_HOST:2181 --create \\\n                    --topic maxwell --partitions 20 --replication-factor 2\n\n\n\n\nhttp://kafka.apache.org/documentation.html#quickstart\n\n\n\n  jQuery(document).ready(function () {\n    jQuery(\"table\").addClass(\"table table-condensed table-bordered table-hover\");\n  });", 
            "title": "Kafka"
        }, 
        {
            "location": "/kafka/#kafka-options", 
            "text": "Any options given to Maxwell that are prefixed with  kafka.  will be passed directly into the Kafka producer configuration\n(with  kafka.  stripped off).  We use the \"new producer\" configuration, as described here: http://kafka.apache.org/documentation.html#newproducerconfigs  Maxwell sets the following Kafka options by default, but you can override them in  config.properties .   kafka.acks = 1  kafka.compression.type = gzip   Maxwell writes to a kafka topic named \"maxwell\" by default.  This can be changed with the  kafka_topic  option.", 
            "title": "Kafka options"
        }, 
        {
            "location": "/kafka/#kafka-key", 
            "text": "Maxwell generates keys for its Kafka messages based upon a mysql row's primary key in JSON format:  {  database : test_tb , table : test_tbl , pk.id :4, pk.part2 : hello }  This key is designed to co-operate with Kafka's log compaction, which will save the last-known\nvalue for a key, allowing Maxwell's Kafka stream to retain the last-known value for a row and act\nas a source of truth.", 
            "title": "Kafka key"
        }, 
        {
            "location": "/kafka/#partitioning", 
            "text": "A binlog event's partition is determined by the selected hash function and hash string as follows    HASH_FUNCTION(HASH_STRING) % TOPIC.NUMBER_OF_PARTITIONS  The HASH_FUNCTION is either java's  hashCode  or  murmurhash3 . The default HASH_FUNCTION is  hashCode . Murmurhash3 may be set with the  kafka_partition_hash  option. The seed value for the murmurhash function is hardcoded to 25342 in the MaxwellKafkaPartitioner class.  The HASH_STRING may be ( database ,  table ,  primary_key ).  The default HASH_STRING is the  database . The partitioning field can be configured using the  kafka_partition_by  option.      Maxwell will discover the number of partitions in its kafka topic upon boot.  This means that you should pre-create your kafka topics,\nand with at least as many partitions as you have logical databases:  bin/kafka-topics.sh --zookeeper ZK_HOST:2181 --create \\\n                    --topic maxwell --partitions 20 --replication-factor 2  http://kafka.apache.org/documentation.html#quickstart  \n  jQuery(document).ready(function () {\n    jQuery(\"table\").addClass(\"table table-condensed table-bordered table-hover\");\n  });", 
            "title": "Partitioning"
        }, 
        {
            "location": "/dataformat/", 
            "text": "How Maxwell translates different mysql types\n\n\n\nstrings (varchar, text)\n\n\n\n\nMaxwell currently supports latin1 and utf-8 columns, and will convert both to UTF-8 before outputting as JSON.\n\n\nblob (+ binary encoded strings)\n\n\n\n\nMaxell will base64 encode BLOB, BINARY and VARBINARY columns (as well as varchar/string columns with a BINARY encoding).\n\n\ndatetime\n\n\n\n\nDatetime columns are output as \"YYYY-MM-DD hh:mm::ss\" strings.  Note that mysql\nhas no problem storing invalid datetimes like \"0000-00-00 00:00:00\", and\nMaxwell chooses to reproduce these invalid datetimes faithfully,\nfor lack of something better to do.\n\n\nmysql\n    create table test_datetime ( id int(11), dtcol datetime );\nmysql\n    insert into test_datetime set dtcol='0000-00-00 00:00:00';\n\n\nmaxwell  {\ntable\n:\ntest_datetime\n,\ntype\n:\ninsert\n,\ndata\n:{\ndtcol\n:\n0000-00-00 00:00:00\n}}\n\n\n\n\nsets\n\n\n\n\noutput as JSON arrays.\n\n\nmysql\n   create table test_sets ( id int(11), setcol set('a_val', 'b_val', 'c_val') );\nmysql\n   insert into test_sets set setcol = 'b_val,c_val';\n\n\nmaxwell {\ntable\n:\ntest_sets\n,\ntype\n:\ninsert\n,\ndata\n:{\nsetcol\n:[\nb_val\n,\nc_val\n]}}", 
            "title": "Data Format"
        }, 
        {
            "location": "/dataformat/#strings-varchar-text", 
            "text": "Maxwell currently supports latin1 and utf-8 columns, and will convert both to UTF-8 before outputting as JSON.", 
            "title": "strings (varchar, text)"
        }, 
        {
            "location": "/dataformat/#blob-binary-encoded-strings", 
            "text": "Maxell will base64 encode BLOB, BINARY and VARBINARY columns (as well as varchar/string columns with a BINARY encoding).", 
            "title": "blob (+ binary encoded strings)"
        }, 
        {
            "location": "/dataformat/#datetime", 
            "text": "Datetime columns are output as \"YYYY-MM-DD hh:mm::ss\" strings.  Note that mysql\nhas no problem storing invalid datetimes like \"0000-00-00 00:00:00\", and\nMaxwell chooses to reproduce these invalid datetimes faithfully,\nfor lack of something better to do.  mysql     create table test_datetime ( id int(11), dtcol datetime );\nmysql     insert into test_datetime set dtcol='0000-00-00 00:00:00'; maxwell  { table : test_datetime , type : insert , data :{ dtcol : 0000-00-00 00:00:00 }}", 
            "title": "datetime"
        }, 
        {
            "location": "/dataformat/#sets", 
            "text": "output as JSON arrays.  mysql    create table test_sets ( id int(11), setcol set('a_val', 'b_val', 'c_val') );\nmysql    insert into test_sets set setcol = 'b_val,c_val'; maxwell { table : test_sets , type : insert , data :{ setcol :[ b_val , c_val ]}}", 
            "title": "sets"
        }, 
        {
            "location": "/bootstrapping/", 
            "text": "Using the maxwell-bootstrap utility\n\n\n\n\nYou can use the \nmaxwell-bootstrap\n utility to bootstrap tables from the command-line.\n\n\n\n\n\n\n\n\noption\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\n--log_level LOG_LEVEL\n\n\nlog level (DEBUG, INFO, WARN or ERROR)\n\n\n\n\n\n\n--user USER\n\n\nmysql username\n\n\n\n\n\n\n--password PASSWORD\n\n\nmysql password\n\n\n\n\n\n\n--host HOST\n\n\nmysql host\n\n\n\n\n\n\n--port PORT\n\n\nmysql port\n\n\n\n\n\n\n--database DATABASE\n\n\nmysql database containing the table to bootstrap\n\n\n\n\n\n\n--table TABLE\n\n\nmysql table to bootstrap\n\n\n\n\n\n\n\n\nUsing the maxwell.bootstrap table\n\n\n\n\nAlternatively you can insert a row in the \nmaxwell.bootstrap\n table to trigger a bootstrap.\n\n\nmysql\n insert into maxwell.bootstrap (database_name, table_name) values ('fooDB', 'barTable');\n\n\n\n\nAsync vs Sync bootstrapping\n\n\n\n\nThe Maxwell replicator is single threaded; events are captured by one thread from the binlog and replicated to Kafka one message at a time.\nWhen running Maxwell with \n--bootstrapper=sync\n, the same thread is used to do bootstrapping, meaning that all binlog events are blocked until bootstrapping is complete.\nRunning Maxwell with \n--bootstrapper=async\n however, will make Maxwell spawn a separate thread for bootstrapping.\nIn this async mode, non-bootstrapped tables are replicated as normal by the main thread, while the binlog events for bootstrapped tables are queued and sent to the replication stream at the end of the bootstrap process.\n\n\nBootstrapping Data Format\n\n\n\n\n\n\na bootstrap starts with a document with \ntype = \"bootstrap-start\"\n\n\nthen documents with \ntype = \"insert\"\n (one per row in the table)\n\n\nthen one document per \nINSERT\n, \nUPDATE\n or \nDELETE\n that occurred since the beginning of bootstrap\n\n\nfinally a document with \ntype = \"bootstrap-complete\"\n\n\n\n\nHere's a complete example:\n\n\nmysql\n create table fooDB.barTable(txt varchar(255));\nmysql\n insert into fooDB.barTable (txt) values (\nhello\n), (\nbootstrap!\n);\nmysql\n insert into maxwell.bootstrap (database_name, table_name) values (\nfooDB\n, \nbarTable\n);\n\n\n\n\nCorresponding replication stream output of table \nfooDB.barTable\n:\n\n\n{\ndatabase\n:\nfooDB\n,\ntable\n:\nbarTable\n,\ntype\n:\ninsert\n,\nts\n:1450557598,\nxid\n:13,\ndata\n:{\ntxt\n:\nhello\n}}\n{\ndatabase\n:\nfooDB\n,\ntable\n:\nbarTable\n,\ntype\n:\ninsert\n,\nts\n:1450557598,\nxid\n:13,\ndata\n:{\ntxt\n:\nbootstrap!\n}}\n{\ndatabase\n:\nfooDB\n,\ntable\n:\nbarTable\n,\ntype\n:\nbootstrap-start\n,\nts\n:1450557744,\ndata\n:{}}\n{\ndatabase\n:\nfooDB\n,\ntable\n:\nbarTable\n,\ntype\n:\nbootstrap-insert\n,\nts\n:1450557744,\ndata\n:{\ntxt\n:\nhello\n}}\n{\ndatabase\n:\nfooDB\n,\ntable\n:\nbarTable\n,\ntype\n:\nbootstrap-insert\n,\nts\n:1450557744,\ndata\n:{\ntxt\n:\nbootstrap!\n}}\n{\ndatabase\n:\nfooDB\n,\ntable\n:\nbarTable\n,\ntype\n:\nbootstrap-complete\n,\nts\n:1450557744,\ndata\n:{}}\n\n\n\n\n\n  jQuery(document).ready(function () {\n    jQuery(\"table\").addClass(\"table table-condensed table-bordered table-hover\");\n  });", 
            "title": "Bootstrapping"
        }, 
        {
            "location": "/bootstrapping/#using-the-maxwell-bootstrap-utility", 
            "text": "You can use the  maxwell-bootstrap  utility to bootstrap tables from the command-line.     option  description      --log_level LOG_LEVEL  log level (DEBUG, INFO, WARN or ERROR)    --user USER  mysql username    --password PASSWORD  mysql password    --host HOST  mysql host    --port PORT  mysql port    --database DATABASE  mysql database containing the table to bootstrap    --table TABLE  mysql table to bootstrap", 
            "title": "Using the maxwell-bootstrap utility"
        }, 
        {
            "location": "/bootstrapping/#using-the-maxwellbootstrap-table", 
            "text": "Alternatively you can insert a row in the  maxwell.bootstrap  table to trigger a bootstrap.  mysql  insert into maxwell.bootstrap (database_name, table_name) values ('fooDB', 'barTable');", 
            "title": "Using the maxwell.bootstrap table"
        }, 
        {
            "location": "/bootstrapping/#async-vs-sync-bootstrapping", 
            "text": "The Maxwell replicator is single threaded; events are captured by one thread from the binlog and replicated to Kafka one message at a time.\nWhen running Maxwell with  --bootstrapper=sync , the same thread is used to do bootstrapping, meaning that all binlog events are blocked until bootstrapping is complete.\nRunning Maxwell with  --bootstrapper=async  however, will make Maxwell spawn a separate thread for bootstrapping.\nIn this async mode, non-bootstrapped tables are replicated as normal by the main thread, while the binlog events for bootstrapped tables are queued and sent to the replication stream at the end of the bootstrap process.", 
            "title": "Async vs Sync bootstrapping"
        }, 
        {
            "location": "/bootstrapping/#bootstrapping-data-format", 
            "text": "a bootstrap starts with a document with  type = \"bootstrap-start\"  then documents with  type = \"insert\"  (one per row in the table)  then one document per  INSERT ,  UPDATE  or  DELETE  that occurred since the beginning of bootstrap  finally a document with  type = \"bootstrap-complete\"   Here's a complete example:  mysql  create table fooDB.barTable(txt varchar(255));\nmysql  insert into fooDB.barTable (txt) values ( hello ), ( bootstrap! );\nmysql  insert into maxwell.bootstrap (database_name, table_name) values ( fooDB ,  barTable );  Corresponding replication stream output of table  fooDB.barTable :  { database : fooDB , table : barTable , type : insert , ts :1450557598, xid :13, data :{ txt : hello }}\n{ database : fooDB , table : barTable , type : insert , ts :1450557598, xid :13, data :{ txt : bootstrap! }}\n{ database : fooDB , table : barTable , type : bootstrap-start , ts :1450557744, data :{}}\n{ database : fooDB , table : barTable , type : bootstrap-insert , ts :1450557744, data :{ txt : hello }}\n{ database : fooDB , table : barTable , type : bootstrap-insert , ts :1450557744, data :{ txt : bootstrap! }}\n{ database : fooDB , table : barTable , type : bootstrap-complete , ts :1450557744, data :{}}  \n  jQuery(document).ready(function () {\n    jQuery(\"table\").addClass(\"table table-condensed table-bordered table-hover\");\n  });", 
            "title": "Bootstrapping Data Format"
        }, 
        {
            "location": "/compat/", 
            "text": "Requirements:\n\n\n\n\n\n\nJRE 7 or above\n\n\nmysql 5.1, 5.5, 5.6\n\n\nkafka 0.8.2 or greater\n\n\n\n\nUnsupported configurations\n\n\n\n\n\n\nMysql 5.7 is untested with Maxwell.  GTID replication is known to not function.\n\n\nMaxwell is incompatible with PARTITION tables, as it is unable to parse the SQL.\n\n\n\n\nbinlog_row_image=MINIMAL\n\n\n\n\nAs of 0.16.2, Maxwell supports binlog_row_image=MINIMAL, but it may not be what you want.  It will differ\nfrom normal Maxwell operation in that:\n\n\n\n\nINSERT statements will no longer output a column's default value\n\n\nUPDATE statements will be incomplete; Maxwell outputs as much of the row as given in the binlogs,\n  but \ndata\n will only include what is needed to perform the update (generally, id columns and changed columns).\n  The \nold\n section may or may not be included, depending on the nature of the update.\n\n\nDELETE statements will be incomplete; generally they will only include the primary key.\n\n\n\n\nMaster recovery\n\n\n\n\nCurrently Maxwell is not very smart about master recovery or detecting a promoted slave; if it determines\nthat the server_id has changed between runs, Maxwell will simply delete its old schema cache and binlog position\nand start again.  We plan on improving master recovery in future releases.\n\n\nIf you know the starting position of your new master, you can start the new Maxwell process with the\n\n--init_position\n flag, which will ensure that no gap appears in a master failover.", 
            "title": "Compat / Caveats"
        }, 
        {
            "location": "/compat/#requirements", 
            "text": "JRE 7 or above  mysql 5.1, 5.5, 5.6  kafka 0.8.2 or greater", 
            "title": "Requirements:"
        }, 
        {
            "location": "/compat/#unsupported-configurations", 
            "text": "Mysql 5.7 is untested with Maxwell.  GTID replication is known to not function.  Maxwell is incompatible with PARTITION tables, as it is unable to parse the SQL.", 
            "title": "Unsupported configurations"
        }, 
        {
            "location": "/compat/#binlog_row_imageminimal", 
            "text": "As of 0.16.2, Maxwell supports binlog_row_image=MINIMAL, but it may not be what you want.  It will differ\nfrom normal Maxwell operation in that:   INSERT statements will no longer output a column's default value  UPDATE statements will be incomplete; Maxwell outputs as much of the row as given in the binlogs,\n  but  data  will only include what is needed to perform the update (generally, id columns and changed columns).\n  The  old  section may or may not be included, depending on the nature of the update.  DELETE statements will be incomplete; generally they will only include the primary key.", 
            "title": "binlog_row_image=MINIMAL"
        }, 
        {
            "location": "/compat/#master-recovery", 
            "text": "Currently Maxwell is not very smart about master recovery or detecting a promoted slave; if it determines\nthat the server_id has changed between runs, Maxwell will simply delete its old schema cache and binlog position\nand start again.  We plan on improving master recovery in future releases.  If you know the starting position of your new master, you can start the new Maxwell process with the --init_position  flag, which will ensure that no gap appears in a master failover.", 
            "title": "Master recovery"
        }
    ]
}